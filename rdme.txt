RAG_PROJECT

A production-focused Retrieval-Augmented Generation (RAG) pipeline designed with hybrid retrieval, 
re ranking, and semantic-aware chunking. The system is built to prioritize accuracy, latency control,
and clean output discipline rather than experimentation or debugging convenience.

==================================================
ğŸ”§ Repository Setup (Required)

This project uses git submodules (e.g. llama.cpp). You must initialize them after cloning.

Option 1: Clone with submodules (recommended)
git clone --recurse-submodules <https://github.com/codee-geek/RAG_Project.git>

Option 2: Initialize submodules after cloning
git clone <yhttps://github.com/codee-geek/RAG_Project.git>
cd <RAG_Project>
git submodule update --init --recursive


âš ï¸ Important: If you skip this step, the llama.cpp/ directory will be empty and the project will not work.

--------------------------------------------------
PROJECT STRUCTURE
--------------------------------------------------

RAG_ATHARVA/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ loader.py
â”‚   â”‚   â”œâ”€â”€ cleaner.py
â”‚   â”‚   â”œâ”€â”€ chunker.py
â”‚   â”‚   â”œâ”€â”€ index.py
â”‚   â”‚
â”‚   â”œâ”€â”€ query/
â”‚   â”‚   â””â”€â”€ pipeline.py      # renamed from Query/user_input.py
â”‚   â”‚
â”‚   â”œâ”€â”€ llm.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ rag.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ main.py               # CLI entry
â”œâ”€â”€ ingest.py             # Calls core.ingestion
â”œâ”€â”€ vectorstore/          # persisted FAISS data (NOT code)
â”œâ”€â”€ setup.sh
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ llama.cpp/
â””â”€â”€ README.md


--------------------------------------------------
HOW TO RUN
--------------------------------------------------

venv: source .venv/bin/activate

Ingestion (build vector store):
python3 -m src.ingestion.ingestion

Querying:
python3 -m src.Query.user_input

streamlit app: streamlit run ui/streamlit_app.py

sample query: What shall organization do when nonconformity occurs? 

--------------------------------------------------
DEPENDENCY MANAGEMENT
--------------------------------------------------

Currently, the project includes:
- requirements.txt
- pyproject.toml (UV-compatible)

This is a temporary setup for ease of development.

Final state (after stabilization):
- requirements.txt will be removed
- All dependencies will be declared only in pyproject.toml
- UV will be the single dependency manager

--------------------------------------------------
RETRIEVAL PIPELINE
--------------------------------------------------

STAGE 1: HYBRID RETRIEVAL
- Retrieve 20â€“50 candidate chunks
- Combination of:
  - Vector search (semantic similarity)
  - BM25 (keyword-based retrieval)

Purpose:
- Reduce semantic-only blind spots
- Capture structurally or keyword-important sections

--------------------------------------------------
STAGE 2: RERANKING
--------------------------------------------------

Candidates are passed to a reranker (e.g., BGE Reranker or Cohere Rerank).

Latency budget (strict):
- Cross-encoder: 150â€“400 ms for 20â€“50 documents
- Hybrid reranker: 80â€“250 ms
- Bi-encoder rerank: 5â€“20 ms

If reranking alone exceeds 500 ms, the system is considered too slow for real-time usage.

------------------------------------------------

--------------------------------------------------
FINAL GENERATION
--------------------------------------------------

- Only top 1â€“3 chunks are passed to the LLM
- LLM generates a single, coherent answer
- Raw chunks are never exposed to the end user

This is intentional and non-negotiable.

--------------------------------------------------
CHUNKING STRATEGY (CRITICAL DESIGN)
--------------------------------------------------

Problem:
Default unstructured chunking splits text abruptly, even when content belongs to the same semantic section. This leads to fragmented embeddings, context loss, and weaker reranking.

Solution: Semantic-Aware Buffered Chunking

Pipeline Flow:
Unstructured.partition()          â† semantic structure inferred here
  â†’ LangChain Documents
    â†’ Cleaning / normalization
      â†’ Chunking (recursive + semantic merging)
        â†’ Embeddings
          â†’ FAISS

Chunking rules:
- Merge text within the same semantic section
- Maintain a rolling buffer
- Flush the buffer only when:
  1. The semantic section changes, or
  2. The buffer reaches a minimum/maximum character threshold to cap size

This preserves meaning without producing oversized or context-less chunks.

--------------------------------------------------
OUTPUT DISCIPLINE
--------------------------------------------------

- No debug prints in production
- No raw chunk dumps
- No â€œtop-N chunkâ€ exposure
- User sees only the final synthesized answer

Anything else is debugging behavior, not product behavior.

--------------------------------------------------
SYSTEM GOALS
--------------------------------------------------

- Accuracy over verbosity
- Latency-aware reranking
- Semantic integrity during chunking
- Deterministic, controlled output

This pipeline is designed for real use, not demos or experimentation.
